{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tma(url, suffix):\n",
    "    url_with_suffix = url + suffix + '.html'\n",
    "    text = ''\n",
    "    r = requests.get(url_with_suffix)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    transcript = soup.find('div', class_='entry-content')  # get the transcript, not marginal content\n",
    "    lines = transcript.find_all(['h4', 'p'])  # h5 is sound effects; ignore.\n",
    "    this_line = None\n",
    "    for line in lines:\n",
    "        # print(line)\n",
    "        if line.name == 'h4':  # h4 == name; new speaker\n",
    "            if this_line:\n",
    "                text += this_line + '\\n' # deposit last line so we can start a new one\n",
    "            try:\n",
    "                this_line = line.string + ': '\n",
    "            except TypeError:\n",
    "                continue\n",
    "        elif line.name == 'p':  # p == speech\n",
    "            try:\n",
    "                this_line += line.string + ' '\n",
    "            except TypeError:   #more than one child -> line.string is None -> error\n",
    "                # this happens if there's a tag like <em> within a <p>.\n",
    "                this_line += ' '.join([child for child in line.children if isinstance(child, str)])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_arsparadoxica(url, suffix):\n",
    "    url_with_suffix = url + suffix\n",
    "    text = ''\n",
    "    r = requests.get(url_with_suffix)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    lines = soup.main.find_all('p')\n",
    "    out_lines = []\n",
    "    # this one's easy; each line is a separate <p>, and is already formatted the way we want it to be.\n",
    "    for line in lines:\n",
    "        if line.string is None:  # has child, like <em></em>, in addition to normal text\n",
    "            this_line = []\n",
    "            for child in line.children:\n",
    "                if isinstance(child, str):\n",
    "                    this_line.append(child)\n",
    "                elif child.string:\n",
    "                    this_line.append(child.string)\n",
    "            out_lines.append(' '.join(this_line))\n",
    "        # if a line starts and ends with [], it's sfx and should not be included\n",
    "        elif not (line.string.startswith('[') and line.string.endswith(']')):\n",
    "            out_lines.append(line.string)\n",
    "    return '\\n'.join(out_lines).replace(':  ', ': ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_batch(url, suffixes, scrape_fn, dir_name=None):\n",
    "    if dir_name is None:\n",
    "        dir_name = scrape_fn.__name__\n",
    "    os.makedirs(dir_name, exist_ok=True)\n",
    "    for suffix in suffixes:\n",
    "        text = scrape_fn(url, suffix)\n",
    "        text = text.replace('…', '...').replace('’', '\\'').replace('“', '\"').replace('”', '\"').replace('‘', '\\'').replace('–','-')\n",
    "        with open(os.path.join(dir_name, suffix) + '.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tma_nums = ['00' + str(i) for i in range(1, 10)] + ['0' + str(i) for i in range(10, 100)] + [str(i) for i in range(100, 201)]\n",
    "scrape_batch('https://snarp.github.io/magnus_archives_transcripts/episode/', tma_nums, scrape_tma, dir_name='tma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the permissions on their website are weird, so i had to run this twice on a few of the transcripts.\n",
    "arspara_nums = ['01', '02', '03-1', '03-2'] + ['0' + str(i) for i in range(4,10)] + ['10-1', '10-2'] + [str(i) for i in range(11, 22)] + ['recorder'] + [str(i) for i in range(22, 36)]\n",
    "scrape_batch('https://arsparadoxica.com/transcript/', arspara_nums, scrape_fn=scrape_arsparadoxica, dir_name='ars_paradoxica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in os.listdir('ars_paradoxica'):\n",
    "    with open(os.path.join('ars_paradoxica', f), 'r', encoding='utf-8') as fi:\n",
    "        text = fi.read()\n",
    "    text = text.replace('…', '...').replace('’', '\\'').replace('“', '\"').replace('”', '\"').replace('‘', '\\'').replace('–','-')\n",
    "    with open(os.path.join('ars_paradoxica', f), 'w', encoding='utf-8') as fi:\n",
    "        fi.write(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the TMA scripts, note that ARCHIVIST (STATEMENT) != ARCHIVIST; the archivist is reading someone else's writing. I'm going to save them under 'STATEMENT'. Ideally, we'd save each statement under the statement-giver's name, but (1) that's one more thing we have to extract, and (2) almost all characters give only one statement, so that wouldn't be very informative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
